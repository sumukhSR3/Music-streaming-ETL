{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your ETL output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neccessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "                     .config(\"spark.jars.packages\",\"org.apache.hadoop:hadoop-aws:2.7.0\")\\\n",
    "                     .appName(\"demo\")\\\n",
    "                     .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure AWS credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 1\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['KEY']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['SECRET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option 2\n",
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "aws_id = config['AWS']['KEY']\n",
    "aws_secret = config['AWS']['SECRET']\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", aws_id)\n",
    "spark.sparkContext._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", aws_secret)\n",
    "input_data = config['S3']['S3_BUCKET_OUTPUT_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = config['S3']['S3_BUCKET_OUTPUT_PATH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_path = os.path.join(input_data, 'songplays')\n",
    "songplays_df = spark.read.parquet(songplays_path)\n",
    "songs_path = os.path.join(input_data, 'songs')\n",
    "songs_df = spark.read.parquet(songs_path)\n",
    "artists_path = os.path.join(input_data, 'artists')\n",
    "artists_df = spark.read.parquet(artists_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_df.createOrReplaceTempView(\"songplays\")\n",
    "songs_df.createOrReplaceTempView(\"songs\")\n",
    "artists_df.createOrReplaceTempView(\"artists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 1: The most popular songs over the time (considering your schema name is sparkify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "          SELECT s.title, count(*) as count\n",
    "            FROM songplays sp\n",
    "            INNER JOIN songs s ON s.song_id = sp.song_id\n",
    "            GROUP BY s.title\n",
    "            ORDER BY count DESC, s.title ASC\n",
    "          '''\n",
    "          ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query 2: The most popular artists and their songs over the time (considering your schema name is sparkify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "          SELECT ar.name, s.title, count(*) as count\n",
    "            FROM songplays sp\n",
    "            INNER JOIN songs s ON s.song_id = sp.song_id\n",
    "            INNER JOIN artists ar ON ar.artist_id = sp.artist_id\n",
    "            GROUP BY ar.name, s.title\n",
    "            ORDER BY count DESC, ar.name, s.title ASC;\n",
    "          ''').show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
