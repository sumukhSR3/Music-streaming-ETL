{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Nasa logs and PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "session = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "dfLog = session.read.text(\"data/NASA_access_log_Jul95.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfLog.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1891715"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfLog.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different ways to observe data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|199.72.81.55 - - ...|\n",
      "|unicomp6.unicomp....|\n",
      "|199.120.110.21 - ...|\n",
      "|burger.letters.co...|\n",
      "|199.120.110.21 - ...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfLog.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLog.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLog.limit(20).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parse Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson on RegEx Functions\n",
    "The **re** module offers a set of functions that allows us to search a string for a match:\n",
    "\n",
    "**Function Description**  \n",
    "findall - Returns a list containing all matches  \n",
    "search - Returns a Match object if there is a match anywhere in the string  \n",
    "split - Returns a list where the string has been split at each match  \n",
    "sub - Replaces one or many matches with a string\n",
    "\n",
    "**Metacharacters**\n",
    "Metacharacters are characters with a special meaning:\n",
    "\n",
    "**Character\tDescription\tExample\tTry it**  \n",
    "[]\t- A set of characters\t\"[a-m]\"\t\n",
    "\\\t- Signals a special sequence (can also be used to escape special characters)\t\"\\d\"\t\n",
    ".\t- Any character (except newline character)\t\"he..o\"\t\n",
    "\\^\t- Starts with\t\\\"\\^hello\\\"\t \n",
    "\\\\$\t- Ends with\t\"world\\\\$\"\t\n",
    "\\*\t- Zero or more occurrences\t\"aix*\"\t\n",
    "\\+\t- One or more occurrences\t\"aix+\"\t\n",
    "{}\t- Exactly the specified number of occurrences\t\"al{2}\"\t\n",
    "|\t- Either or\t\"falls|stays\"\t\n",
    "()\t- Capture and group\t \t \n",
    "Special Sequences - \n",
    "A special sequence is a \\\\ followed by one of the characters in the list below, and has a special meaning:\n",
    "\n",
    "**Character\tDescription\tExample\tTry it:**  \n",
    "\\A\t- Returns a match if the specified characters are at the beginning of the string\t\"\\AThe\"\t\n",
    "\\b\t- Returns a match where the specified characters are at the beginning or at the end of a word\tr\"\\bain\"\n",
    "r\"ain\\b\"  \t\n",
    "\\B\t- Returns a match where the specified characters are present, but NOT at the beginning (or at the end) of a word\tr\"\\Bain\"\n",
    "r\"ain\\B\" - \t\n",
    "\\d\t- Returns a match where the string contains digits (numbers from 0-9)\t\"\\d\"\t\n",
    "\\D\t- Returns a match where the string DOES NOT contain digits\t\"\\D\"\t\n",
    "\\s\t- Returns a match where the string contains a white space character\t\"\\s\"\t\n",
    "\\S\t- Returns a match where the string DOES NOT contain a white space character\t\"\\S\"\t\n",
    "\\w\t- Returns a match where the string contains any word characters (characters from a to Z, digits from 0-9, and the underscore _ character)\t\"\\w\"\t\n",
    "\\W\t- Returns a match where the string DOES NOT contain any word characters\t\"\\W\"\t\n",
    "\\Z\t- Returns a match if the specified characters are at the end of the string\t\"Spain\\Z\"\t\n",
    "Sets - A set is a set of characters inside a pair of square brackets [] with a special meaning:\n",
    "\n",
    "**Set\tDescription\tTry it**\n",
    "[arn]\tReturns a match where one of the specified characters (a, r, or n) are present\t\n",
    "[a-n]\tReturns a match for any lower case character, alphabetically between a and n\t\n",
    "[^arn]\tReturns a match for any character EXCEPT a, r, and n\t\n",
    "[0123]\tReturns a match where any of the specified digits (0, 1, 2, or 3) are present\t\n",
    "[0-9]\tReturns a match for any digit between 0 and 9\t\n",
    "[0-5][0-9]\tReturns a match for any two-digit numbers from 00 and 59\t\n",
    "[a-zA-Z]\tReturns a match for any character alphabetically between a and z, lower case OR upper case\t\n",
    "[+]\tIn sets, +, *, ., |, (), $,{} has no special meaning, so [+] means: return a match for any + character in the string\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "import re\n",
    "# 199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
    "@udf(MapType(StringType(), StringType()))\n",
    "def parseUDF(line):\n",
    "    PATTERN = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s+-\\d{4})\\] \"(\\S+) (\\S+)\\s*(\\S*)\" (\\d{3}) (\\S+)'\n",
    "    match = re.search(PATTERN, line)\n",
    "    if match is None:\n",
    "        return (line, 0)\n",
    "    size_field = match.group(9)\n",
    "    if size_field == '-':\n",
    "        size = 0\n",
    "    else:\n",
    "        size = match.group(9)\n",
    "    return {\n",
    "        \"host\": match.group()1),\n",
    "        \"client_id\": match.group(2),\n",
    "        \"user_id\": match.group(3),\n",
    "        \"date_time\": match.group(4),\n",
    "        \"method\": match.group(5),\n",
    "        \"endpoint\": match.group(6),\n",
    "        \"protocol\": match.group(7),\n",
    "        \"response_code\": int(match.group(8)),\n",
    "        \"content_size\": size\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfParsed = dfLog.withColumn(\"parsed\", parseUDF(\"value\"))\n",
    "dfParsed.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- parsed: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfParsed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfParsed.selectExpr(\"parsed['host'] as host\", \"parsed['date_time'] as datetime\").limit(5).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\"host\", \"client_id\", \"user_id\", \"date_time\", \"method\", \"endpoint\", \"protocol\", \"response_code\", \"content_size\"]\n",
    "exprs = [\"parsed['{}'] as {}\".format(field, field) for field in fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean = dfParsed.selectExpr(*exprs)\n",
    "dfClean = dfClean.filter(dfClean.date_time.isNotNull())\n",
    "dfClean.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "dfClean.groupBy(\"host\").count().orderBy(desc(\"count\")).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "dfClean.groupBy(\"endpoint\").count().orderBy(desc(\"count\")).limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting: Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean.createOrReplaceTempView(\"dfCleanTable\")\n",
    "session.sql(\"\"\"\n",
    "            SELECT DISTINCT endpoint, cast(content_size as int) \n",
    "            FROM dfCleanTable\n",
    "            ORDER BY content_size DESC\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting: Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "dfCleanTyped = dfClean.withColumn(\"content_size_int\", expr(\"cast(content_size as int)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCleanTyped.createOrReplaceTempView(\"dfCleanTable\")\n",
    "session.sql(\"\"\"\n",
    "            SELECT endpoint, content_size_int \n",
    "            FROM dfCleanTable\n",
    "            ORDER BY content_size_int DESC\n",
    "            \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "# 01/Jul/1995:00:00:01 -0400\n",
    "@udf\n",
    "def parseDate(line):\n",
    "    pos = line.find('-')\n",
    "    line = line.replace(line[pos:], \"\").strip()\n",
    "    date_part = line.replace(\"-0400\", \"\")\n",
    "    date_part2 = date_part.replace(\"/\", \"-\")\n",
    "    date_part3 = date_part2.strip()\n",
    "    pos1 = date_part3.find(':')\n",
    "    date_part4 = date_part3.replace(date_part3[:pos1+1], date_part3[:pos1] + \" \")\n",
    "    month_dict = {'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04', 'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08', 'Sep': '09', 'Oct': '10', 'Nov': '11', 'Dec': '12'}    \n",
    "    pos2 = date_part4.find(\"-\")\n",
    "    month = date_part4[pos2+1:pos2+4]\n",
    "    date_part5 = date_part4.replace(month, month_dict.get(month))\n",
    "    return date_part5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfClean2 = dfClean.withColumn(\"date_part\", parseDate(\"date_time\"))\n",
    "dfClean2.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import unix_timestamp\n",
    "dfClean2.withColumn(\"date_formatted\", unix_timestamp(dfClean2.date_part, 'dd-MM-yyyy HH:mm:ss')).limit(20).orderBy(\"date_formatted\").toPandas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
